{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction and problem statement\n",
    "\n",
    "Finding a place to live is a big challenge, especially in big cities. Not only it is time consuming, but compare them whn we narrow down our choice to a few is another challenge. For families with children the best school district is priority, for single workers, being closer to a subway is a plus.\n",
    "\n",
    "In this notebook, we will:\n",
    "- Scrape the information from an apartment listing data \n",
    "- Clean and transform the data\n",
    "- Analyze and visualize the data\n",
    "- Forcasting the price for apartment for different neigborhoods\n",
    "\n",
    "#### Scraping Data from the web\n",
    "For this exercise, we will be using RentHopsite(http://www.renthop.com). We will focus our energy only on apartments in NYC and neighorhoods.\n",
    "![title](img/homes.jpg)\n",
    "\n",
    "Above is the overview of the site. We will be retrieving the informations below:\n",
    "- Address\n",
    "- Number of beds and number of baths\n",
    "- The rental listing price\n",
    "- Zip code\n",
    "- apartment description\n",
    "- etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding libraries we will be using\n",
    "\n",
    "For scraping we will be using **requests** to pull down listings and use **BeautifulSoup** to extract different attributes. There are two populars way to navigate  HTML structures known as **Document Object Models (DOMs)** XPath and CSS Selectors. We will be using the CSS selectors, which is a pattern language built to work with HTML and identify elements using a combination of element type, class,and ID properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import requests \n",
    "import matplotlib.pyplot as plt \n",
    "import googlemaps \n",
    "from bs4 import BeautifulSoup\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1- Getting the whole page to scrape\n",
    "  Fist of all, let us define a function call get_url that will allow us to get the Document Object Model for a given page. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_url (url):\n",
    "    try:\n",
    "        response = requests.get(url).content\n",
    "        return BeautifulSoup(response,'html.parser')\n",
    "    except requests.exceptions.RequestException as e: \n",
    "        print(\"Something when wrong !\")\n",
    "        raise SystemExit(e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2- Getting the list of the all listings to scrape\n",
    "After getting the HTML for the whole page, we are going to write another fuction to get the area that has only all the properties that we are interested in.\n",
    "Using Chrome developer console, we see that the main text is within a **div** tag with *id=search-results-list* and each listing is in another **div** tag with *class=search-listing*. therefore, we will be getting all *search-listing*.\n",
    "As we can see, each page has 20 listings.\n",
    "Therefore,we are going to define the locator that will gather all listinds as **APARTMENTS_LOCATOR='div.search-listing'**.\n",
    "All the other locator will follow the same pattern.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_listings(soup, locator):\n",
    "    '''\n",
    "    our function takes a soup object and a locator and return the\n",
    "     list of all individual listings.\n",
    "    '''\n",
    "    return soup.select(locator) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After inspecting all the components we will be scraping, below is the different locators that we will be using to get differents attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here is the list of all locator we have identified\n",
    "\n",
    "APTS_LOCATOR = 'div.search-listing'\n",
    "LOC_LOCATOR  = 'div.search-listing'\n",
    "LINK_LOCATOR = 'div.search-listing a'\n",
    "LISTINGID_LOCATOR = 'div.search-listing'\n",
    "LATITUDE_LOCATOR = 'div.search-listing'\n",
    "LONGITUDE_LOCATOR = 'div.search-listing'\n",
    "ADDR_LOCATOR = 'div.search-listing div.search-info'\n",
    "NAME_LOCATOR = 'div.search-listing div.search-info div.font-size-9.overflow-ellipsis'\n",
    "ADRNAME_LOCATOR = 'div.search-info>div>a'\n",
    "PRICE_LOCATOR = 'div.search-listing  div.search-info div table tr'\n",
    "BROKER_LOCATOR= 'div.search-listing div.search-info'\n",
    "SQFT_LOCATOR  ='div.row.no-gutters div.px-3>div>table'\n",
    "DESC_LOCATOR  = 'div.row.no-gutters div.font-size-10 p'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3- processing each listing to get individual attributes\n",
    "Here we are going to get data for each individual listing. We are going to built different functions that will allow us to get our data.\n",
    "\n",
    "##### a) getting location and link attributes\n",
    "We see that all the locations attributes and Listingid can be obtained using the same locator. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_location(apt, locator=LOC_LOCATOR):\n",
    "    '''\n",
    "      return listingid, longitude, latitude for a given listing\n",
    "    '''\n",
    "    location=apt.select_one(locator)\n",
    "    #print(location)\n",
    "    listingID = apt.attrs['listing_id']\n",
    "    longitude = apt.attrs['longitude']\n",
    "    latitude  = apt.attrs['latitude']\n",
    "        \n",
    "    return {\n",
    "        'listingID':listingID,\n",
    "        'longitude':longitude,\n",
    "        'latitude' :latitude,\n",
    "       }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting the link for the property detail page\n",
    "def get_link(apt,locator=LINK_LOCATOR):\n",
    "    '''\n",
    "    return the link to the detail page for each apartment\n",
    "    '''\n",
    "    link = apt.select_one(locator)\n",
    "    return link.attrs['href']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_address(apt,locator=ADRNAME_LOCATOR):\n",
    "    '''\n",
    "      return the apartment address\n",
    "    '''\n",
    "    return apt.select_one(locator).text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_neighborhood(apt,locator=NAME_LOCATOR):\n",
    "    '''\n",
    "    return the neighborhood for a given listing\n",
    "    '''\n",
    "    name=apt.select_one(locator)\n",
    "    return name.text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_description(soup,locator=DESC_LOCATOR):\n",
    "    '''\n",
    "    return the description for each listing\n",
    "    '''\n",
    "    description=get_all_listings(soup,locator)\n",
    "    if description is not None:\n",
    "        return description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_price_bed_bath(soup_detail,locator=PRICE_LOCATOR):\n",
    "    '''\n",
    "    From the detail page, return the price, the number of bed and bath \n",
    "    for the listing\n",
    "    '''\n",
    "    result={}\n",
    "    price_bath_bed=get_all_listings(soup_detail,locator=SQFT_LOCATOR)[0].text.split('\\n\\n')\n",
    "    price_bath_bed=[b.strip() for b in price_bath_bed if len(b)>0]\n",
    "    if price_bath_bed[0][0]=='$':\n",
    "        price =float(price_bath_bed[0].strip().replace(',','')[1:])\n",
    "        result['price']=price\n",
    "    \n",
    "    n = len(price_bath_bed)\n",
    "    for i in range(1,n):\n",
    "        element = price_bath_bed[i].split(' ')#[0]\n",
    "        if len(element)>1:\n",
    "            result[element[1].strip().replace('\\n/','')]=element[0].strip()\n",
    "        else:\n",
    "            result['Bed']=element[0].strip()\n",
    "    \n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4- Putting all functions together to scrape a page\n",
    "In this section, we are going to put all functions defined above to get all attributes to for apartments on a given page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_page(url):\n",
    "    soup = get_url(url)\n",
    "    apartments=get_all_listings(soup,locator=APTS_LOCATOR)\n",
    "    result=[]\n",
    "    \n",
    "    \n",
    "    for apt in apartments:\n",
    "        apartment={}\n",
    "\n",
    "        #getting location\n",
    "        location = get_location(apt,locator=LOC_LOCATOR)\n",
    "        #getting apartment link\n",
    "        link=get_link(apt,locator=LINK_LOCATOR)\n",
    "        #getting property address\n",
    "        address=get_address(apt,locator=ADRNAME_LOCATOR)\n",
    "        #getting apts name\n",
    "        neighborhood = get_neighborhood(apt,locator=NAME_LOCATOR)\n",
    "        #getting the property detail page\n",
    "        soup_detail=get_url(link)\n",
    "\n",
    "        #getting renting price, apart bed, bath and sqft\n",
    "        apt_detail =get_price_bed_bath(soup_detail,PRICE_LOCATOR)\n",
    "\n",
    "        description = get_description(soup_detail,locator=DESC_LOCATOR)\n",
    "\n",
    "        apartment.update(location)\n",
    "        apartment['link']=link\n",
    "        apartment['address']=address\n",
    "        apartment['neighborhood']=neighborhood\n",
    "        apartment.update(apt_detail)\n",
    "        apartment['description']=description\n",
    "        \n",
    "        result.append(apartment)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that each page for has 20 listings. To get enough data for our analysis, we will scape 100*20 =2000 apartments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_pages_parsed=[]\n",
    "for i in range(1,10):\n",
    "    target_page=f\"https://www.renthop.com/search/nyc?max_price=50000&min_price={i}&sort=hopscore&q=&search=0\"\n",
    "    #print(target_page)\n",
    "    result=scrape_page(target_page)\n",
    "    all_pages_parsed.extend(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets take the final result from our last function and create a pandas dataframe\n",
    "data=pd.DataFrame(all_pages_parsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a view of our data\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding Zip code\n",
    "when dealing with geographic data, it is a import to add zip code, that we do not directly have from the data provided.\n",
    "We can get this information using google API.This is not a free service, but since it gives a $300 credit each month, this will be enought for our purpose.\n",
    "We will sign up to : https://developers.google.com/maps/documentation/geocoding/intro:\n",
    "We can click on **Get Started** on the upper right corner of the page.\n",
    "\n",
    "To get the info needed, make sure to copy and save the API key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting the map\n",
    "gmaps = googlemaps.Client(key='AIzaSyB26Iz6EC3aBOW59CaV7E5SPiOh19REtGM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Example of location\n",
    "ta = data.loc[3,['address']].values[0]+' '+data.loc[3,['neighborhood']].values[0].split(', ')[-1]\n",
    "ta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Example of JSON return\n",
    "geocode_result=gmaps.geocode(ta)\n",
    "geocode_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#how do we get the postal code\n",
    "for p in geocode_result[0]['address_components']:\n",
    "    if 'postal_code' in p['types']:\n",
    "        print(p['short_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_zip(row): \n",
    "    '''\n",
    "    return the zip code for a given neigborhood\n",
    "    '''\n",
    "    try: \n",
    "        allrow = row['address'] + ' ' + row['neighborhood'].split(', ')[-1] \n",
    "        if re.match('^\\d+\\s\\w', allrow): \n",
    "            geocode_result = gmaps.geocode(allrow) \n",
    "            for piece in geocode_result[0]['address_components']: \n",
    "                if 'postal_code' in piece['types']: \n",
    "                    return piece['short_name'] \n",
    "                else: \n",
    "                    continue \n",
    "        else: \n",
    "            return np.nan \n",
    "    except: \n",
    "        return np.nan \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After defining the function to extract the zip code, let's apply to our dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['zip'] = data.apply(get_zip, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A view of our data with zip code.\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "After define this dataframe, this ends the scraping section. We are going to save or dataframe to csv, and continue the analysis on a different dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('apts_with_zip.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#zdf = data[data['zip'].notnull()].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
